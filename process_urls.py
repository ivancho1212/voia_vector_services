from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from .process_custom_texts import process_pending_custom_texts

app = FastAPI(
    title="Voia Vector Services",
    description="API para procesar documentos, URLs y textos planos, generando embeddings y almacenÃ¡ndolos en Qdrant.",
    version="1.1.0"
)

# âœ… ConfiguraciÃ³n de CORS (puedes restringir en producciÃ³n)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Ejemplo: ["http://localhost:3000"] si es frontend local
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# âœ… Endpoint raÃ­z: status de la API
@app.get("/")
def read_root():
    return {
        "status": "ok",
        "message": "ğŸš€ API de Voia Vector Services funcionando correctamente.",
        "endpoints": [
            "/process-documents",
            "/process-urls",
            "/process-custom-texts",
            "/process-all"
        ]
    }


# âœ… Procesar documentos PDF
@app.post("/process-documents/")
def process_documents_endpoint():
    try:
        print("ğŸš€ Iniciando procesamiento de documentos PDF...")
        process_pending_documents()
        return {"status": "ok", "message": "âœ… Documentos procesados exitosamente."}
    except Exception as e:
        print(f"âŒ Error en /process-documents: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# âœ… Procesar URLs (sitios web)
@app.post("/process-urls/")
def process_urls_endpoint():
    try:
        print("ğŸš€ Iniciando procesamiento de URLs...")
        process_pending_urls()
        return {"status": "ok", "message": "âœ… URLs procesadas exitosamente."}
    except Exception as e:
        print(f"âŒ Error en /process-urls: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# âœ… Procesar textos planos
@app.post("/process-custom-texts/")
def process_custom_texts_endpoint():
    try:
        print("ğŸš€ Iniciando procesamiento de textos planos...")
        process_pending_custom_texts()
        return {"status": "ok", "message": "âœ… Textos planos procesados exitosamente."}
    except Exception as e:
        print(f"âŒ Error en /process-custom-texts: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# âœ… Procesar todo: documentos, URLs y textos planos
@app.post("/process-all/")
def process_all_endpoint():
    try:
        print("ğŸš€ Iniciando procesamiento de documentos, URLs y textos planos...")
        process_pending_documents()
        process_pending_urls()
        process_pending_custom_texts()
        return {"status": "ok", "message": "âœ… Documentos, URLs y textos planos procesados exitosamente."}
    except Exception as e:
        print(f"âŒ Error en /process-all: {e}")
        raise HTTPException(status_code=500, detail=str(e))
import mysql.connector
from dotenv import load_dotenv
import os

load_dotenv()

def get_connection():
    return mysql.connector.connect(
        host=os.getenv("DB_HOST"),
        port=os.getenv("DB_PORT"),
        user=os.getenv("DB_USER"),
        password=os.getenv("DB_PASSWORD"),
        database=os.getenv("DB_NAME"),
    )
from sentence_transformers import SentenceTransformer

# Carga del modelo de embeddings
model = SentenceTransformer("all-MiniLM-L6-v2")

def get_embedding(text):
    # Convierte el texto en vector y lo devuelve como lista
    return model.encode(text).tolist()
# voia_vector_services/main.py
from fastapi import FastAPI, Query, HTTPException  # ğŸ‘ˆ Agregar HTTPException
from dotenv import load_dotenv
import os

# Cargar variables de entorno
load_dotenv()
# from process_documents import process_pending_documents # noqa  # Deshabilitado: rompe el flujo por ciclos/imports
# from process_urls import process_pending_urls # noqa
# from process_custom_texts import process_pending_custom_texts # noqa
# from search_vectors import search_vectors # noqa

from fastapi import FastAPI, Query
from pydantic import BaseModel

class SearchRequest(BaseModel):
    query: str
    bot_id: int
    limit: int = 3

# Instancia FastAPI
app = FastAPI()

# Endpoints existentes
@app.get("/process_all")
def process_all():
    print("ğŸš€ Procesando PDFs...")
    process_pending_documents()
    print("ğŸŒ Procesando URLs...")
    process_pending_urls()
    print("ğŸ“ Procesando textos planos...")
    process_pending_custom_texts()
    return {"status": "success", "message": "All processing tasks have been initiated."}

@app.get("/process_documents")
def process_documents_endpoint(bot_id: int = Query(..., description="ID del bot para procesar documentos")):
    try:
        print(f"ğŸš€ Procesando PDFs para el bot_id: {bot_id}...")
        process_pending_documents(bot_id)
        return {"status": "success", "message": f"Document processing initiated for bot {bot_id}."}
    except Exception as e:
        print(f"âŒ Error en process_documents_endpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/process_urls")
def process_urls_endpoint(bot_id: int = Query(..., description="ID del bot para procesar URLs")):
    try:
        print(f"ğŸŒ Procesando URLs para el bot_id: {bot_id}...")
        process_pending_urls(bot_id)
        return {"status": "success", "message": f"URL processing initiated for bot {bot_id}."}
    except Exception as e:
        print(f"âŒ Error en process_urls_endpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/process_texts")
def process_texts_endpoint(bot_id: int = Query(..., description="ID del bot para procesar textos")):
    try:
        print(f"ğŸ“ Procesando textos para el bot_id: {bot_id}...")
        process_pending_custom_texts(bot_id)
        return {"status": "success", "message": f"Custom text processing initiated for bot {bot_id}."}
    except Exception as e:
        print(f"âŒ Error en process_texts_endpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ğŸ”¹ Endpoint para bÃºsqueda de vectores (NUEVO, para el chat dinÃ¡mico)
@app.post("/search")
def search_vectors_endpoint(request: SearchRequest):
    """
    Busca vectores en Qdrant asociados a un bot dado y un query opcional.
    """
    try:
        results = search_vectors(bot_id=request.bot_id, query=request.query, limit=request.limit)
        return {"results": results}
    except Exception as e:
        print(f"âŒ Error en el endpoint /search: {e}")
        # Esto devolverÃ¡ un error 500 al cliente C# con un mensaje especÃ­fico
        raise HTTPException(status_code=500, detail=f"Error interno en el servicio de bÃºsqueda de Python: {str(e)}")

# ğŸ”¹ Endpoint de bÃºsqueda de vectores (ANTIGUO, restaurado para compatibilidad)
@app.get("/search_vectors")
def search_vectors_get_endpoint(
    bot_id: int = Query(..., description="ID del bot"),
    query: str = Query("", description="Texto de bÃºsqueda opcional"),
    limit: int = Query(5, description="Cantidad mÃ¡xima de resultados")
):
    """
    Busca vectores en Qdrant. Mantenido por compatibilidad con flujos existentes.
    """
    try:
        results = search_vectors(bot_id=bot_id, query=query, limit=limit)
        return results
    except Exception as e:
        print(f"âŒ Error en el endpoint /search_vectors: {e}")
        raise HTTPException(status_code=500, detail=f"Error interno en el servicio de bÃºsqueda de Python: {str(e)}")
import uuid
import hashlib
# from db import get_connection
# from vector_store import get_or_create_vector_store
# from embedder import get_embedding
# from tag_utils import infer_tags_from_payload

# client = get_or_create_vector_store()

def process_pending_custom_texts(bot_id: int):
    print(f"ğŸš€ Iniciando procesamiento de textos planos para el bot {bot_id}...")

    conn = get_connection()
    cursor = conn.cursor(dictionary=True)

    try:
        cursor.execute("""
            SELECT id, content, bot_id, bot_template_id, user_id
            FROM training_custom_texts
            WHERE indexed = 0 AND bot_id = %s
        """, (bot_id,))
        texts = cursor.fetchall()

        if not texts:
            print(f"â„¹ï¸ No hay textos planos pendientes por procesar para el bot {bot_id}.")
            return

        for item in texts:
            try:
                content = item['content'].strip()

                if not content:
                    print(f"âš ï¸ Texto vacÃ­o. ID {item['id']}")
                    cursor.execute("UPDATE training_custom_texts SET indexed = -1 WHERE id = %s", (item['id'],))
                    conn.commit()
                    continue

                content_hash = hashlib.sha256(content.encode("utf-8")).hexdigest()

                cursor.execute("""
                    SELECT COUNT(*) as count FROM training_custom_texts
                    WHERE content_hash = %s AND indexed = 1 AND bot_id = %s
                """, (content_hash, bot_id))
                if cursor.fetchone()['count'] > 0:
                    print(f"â© Texto con contenido idÃ©ntico ya fue indexado para este bot. Se omite. ID {item['id']}")
                    cursor.execute("UPDATE training_custom_texts SET indexed = 2 WHERE id = %s", (item['id'],))
                    conn.commit()
                    continue

                qdrant_id = str(uuid.uuid4())

                payload = {
                    "type": "custom_text",
                    "user_id": item.get('user_id'),
                    "bot_id": item.get('bot_id'),
                    "bot_template_id": item.get('bot_template_id'),
                    "source": "training_custom_texts",
                }

                tags = infer_tags_from_payload(payload, content)
                payload.update(tags)

                print(f"ğŸ·ï¸ Etiquetas inferidas: {tags}")

                client.upsert(
                    collection_name="voia_vectors",
                    points=[{
                        "id": qdrant_id,
                        "vector": get_embedding(content),
                        "payload": payload
                    }]
                )

                cursor.execute("""
                    UPDATE training_custom_texts
                    SET indexed = 1, qdrant_id = %s, content_hash = %s
                    WHERE id = %s
                """, (qdrant_id, content_hash, item['id']))

                conn.commit()
                print(f"âœ… Texto plano ID {item['id']} procesado y vectorizado.")

            except Exception as e:
                print(f"âŒ Error procesando texto ID {item['id']}: {e}")
                cursor.execute("UPDATE training_custom_texts SET indexed = -1 WHERE id = %s", (item['id'],))
                conn.commit()
                raise Exception(f"Fallo al procesar el texto (ID: {item['id']}): {e}")

    finally:
        if 'conn' in locals() and conn.is_connected():
            cursor.close()
            conn.close()
        print(f"\nğŸ”š Procesamiento de textos planos para el bot {bot_id} finalizado.")
import os
import uuid
import hashlib
from PyPDF2 import PdfReader
from pdf2image import convert_from_path
import pytesseract
# from db import get_connection
# from vector_store import get_or_create_vector_store
# from embedder import get_embedding
# from tag_utils import infer_tags_from_payload

# client = get_or_create_vector_store()  # Deshabilitado: funciÃ³n no disponible por import

def extract_text_from_pdf(path):
    try:
        reader = PdfReader(path)

        # Verificar si el PDF estÃ¡ cifrado
        if reader.is_encrypted:
            try:
                reader.decrypt("")  # Intenta desbloquear con contraseÃ±a vacÃ­a
            except Exception:
                raise Exception("Archivo PDF protegido con contraseÃ±a, no se puede procesar.")

            if reader.is_encrypted:
                raise Exception("Archivo PDF sigue cifrado, no se puede leer.")

        text = "\n".join(page.extract_text() or '' for page in reader.pages)
        return text.strip()
    except Exception as e:
        print(f"âŒ Error leyendo PDF {path}: {e}")
        raise

def extract_text_from_pdf_with_ocr(path):
    try:
        pages = convert_from_path(path)
        text = ""
        for page in pages:
            text += pytesseract.image_to_string(page)
        return text.strip()
    except Exception as e:
        print(f"âŒ Error OCR PDF {path}: {e}")
        return ""

def index_document(qdrant_id, text, metadata):
    try:
        vector = get_embedding(text)
        client.upsert(
            collection_name="voia_vectors",
            points=[{
                "id": qdrant_id,
                "vector": vector,
                "payload": metadata
            }]
        )
        print(f"âœ… Documento indexado en Qdrant con ID: {qdrant_id}")
    except Exception as e:
        print(f"âŒ Error al indexar en Qdrant: {e}")
        raise

def handle_invalid_pdf(path, doc_id, user_id, cursor, conn):
    print(f"ğŸ—‘ï¸ Eliminando archivo invÃ¡lido: {path}")
    try:
        if os.path.exists(path):
            os.remove(path)
            print("âœ… Archivo eliminado.")
        else:
            print("âš ï¸ Archivo ya no existe.")
    except Exception as e:
        print(f"âŒ Error al eliminar archivo: {e}")

    cursor.execute("UPDATE uploaded_documents SET indexed = -1 WHERE id = %s", (doc_id,))
    conn.commit()
    print(f"ğŸ“¢ Notificar a usuario {user_id} que su archivo es invÃ¡lido.")

def process_pending_documents(bot_id: int):
    conn = get_connection()
    cursor = conn.cursor(dictionary=True)

    try:
        cursor.execute("""
            SELECT id, file_path, bot_id, bot_template_id, user_id, file_name 
            FROM uploaded_documents 
            WHERE indexed = 0 AND bot_id = %s
        """, (bot_id,))
        documents = cursor.fetchall()

        if not documents:
            print(f"âœ… No hay documentos pendientes para el bot {bot_id}.")
            return

        for doc in documents:
            # Corregir escape en rutas
            # Usar la ruta base configurada o inferirla
            net_root = os.getenv('DOTNET_ROOT_PATH', 'C:/Users/Ivan Herrera/Documents/VIA/Api')
            relative_path = doc['file_path'].replace('\\', '/')
            abs_path = os.path.normpath(os.path.join(net_root, relative_path))

            print(f"ğŸ“„ Procesando: {abs_path} para el bot {bot_id}")
            print(f"ğŸ” Detalles de la ruta:")
            print(f"  - NET_ROOT: {net_root}")
            print(f"  - Ruta relativa: {relative_path}")
            print(f"  - Ruta absoluta: {abs_path}")
            print(f"  - Â¿Archivo existe?: {os.path.exists(abs_path)}")

            try:
                content = extract_text_from_pdf(abs_path)

                if not content.strip():
                    print("âš ï¸ Texto vacÃ­o, intentando OCR...")
                    content = extract_text_from_pdf_with_ocr(abs_path)

                if content.strip():
                    print(f"âœ… Texto extraÃ­do: {content[:300]} ...")
                    content_hash = hashlib.sha256(content.encode("utf-8")).hexdigest()

                    cursor.execute("""
                        SELECT COUNT(*) as count FROM uploaded_documents
                        WHERE content_hash = %s AND indexed = 1 AND bot_id = %s
                    """, (content_hash, bot_id))
                    if cursor.fetchone()['count'] > 0:
                        print("â© Documento con contenido idÃ©ntico ya fue indexado para este bot. Se omite.")
                        cursor.execute("UPDATE uploaded_documents SET indexed = 2 WHERE id = %s", (doc['id'],))
                        conn.commit()
                        continue

                    qdrant_id = str(uuid.uuid4())

                    # âœ… METADATA COMPLETO para mejor trazabilidad
                    base_payload = {
                        # Identificadores
                        "doc_id": doc['id'],
                        "bot_id": doc['bot_id'],
                        "user_id": doc['user_id'],
                        "bot_template_id": doc['bot_template_id'],
                        
                        # Fuente y tipo
                        "source": "url",  # "document", "url", "text"
                        "url": doc['file_path'],  # URL guardada
                        "file_name": doc['file_name'],
                        
                        # Contenido
                        "original_text": content[:500],  # Primeros 500 chars para preview
                        "text_length": len(content),
                        "chunk_number": 1,
                        
                        # Metadata
                        "processed_at": __import__('datetime').datetime.now().isoformat(),
                        "content_hash": content_hash,
                    }
                    
                    # Agregar tags inferidos
                    tags = infer_tags_from_payload(base_payload, content)
                    payload = {**base_payload, **tags}

                    index_document(qdrant_id, content, payload)

                    cursor.execute("""
                        UPDATE uploaded_documents 
                        SET indexed = 1, qdrant_id = %s, content_hash = %s, extracted_text = %s
                        WHERE id = %s
                    """, (qdrant_id, content_hash, content[:10000], doc['id']))

                    conn.commit()
                else:
                    print("âŒ No se pudo extraer texto del archivo (vacÃ­o o ilegible).")
                    handle_invalid_pdf(abs_path, doc['id'], doc['user_id'], cursor, conn)

            except Exception as e:
                # âœ… MANEJO DE ERROR: marcar como fallido (indexed = -1) para evitar ciclo infinito
                print(f"âŒ Error al procesar el archivo: {e}")
                try:
                    cursor.execute("""
                        UPDATE uploaded_documents 
                        SET indexed = -1, error_message = %s
                        WHERE id = %s
                    """, (str(e)[:500], doc['id']))
                    conn.commit()
                    print(f"ğŸ“Œ Documento {doc['id']} marcado como FALLIDO (indexed=-1)")
                except Exception as db_error:
                    print(f"âŒ Error actualizando DB: {db_error}")
                    conn.rollback()
                # Continuar con siguiente documento en lugar de fallar
                continue

    finally:
        cursor.close()
        conn.close()
import uuid
import hashlib
from datetime import datetime
# from db import get_connection
# from vector_store import get_or_create_vector_store
# from embedder import get_embedding
from .services.document_processor import process_url
from .text_chunking import split_into_chunks
# from tag_utils import infer_tags_from_payload

def process_pending_urls(bot_id: int):
    """
    âœ… SOLUTION #3 + #4: Procesamiento de URLs con error handling robusto y metadata completo.
    
    CaracterÃ­sticas:
    - Try-catch INDIVIDUAL por URL
    - Actualiza indexed = -1 en error
    - ContinÃºa batch incluso si una URL falla
    - Metadata COMPLETO en cada resultado
    """
    print(f"ğŸš€ Iniciando procesamiento de URLs pendientes para el bot {bot_id}...")

    conn = get_connection()
    cursor = conn.cursor(dictionary=True)
    
    processed_count = 0
    failed_count = 0

    try:
        cursor.execute("""
            SELECT id, url, bot_id, bot_template_id, user_id 
            FROM training_urls 
            WHERE indexed = 0 AND status = 'pending' AND bot_id = %s
        """, (bot_id,))
        urls = cursor.fetchall()

        if not urls:
            print(f"â„¹ï¸ No hay URLs pendientes por procesar para el bot {bot_id}.")
            return

        client = get_or_create_vector_store()

        for url_item in urls:
            url_id = url_item['id']
            url = url_item.get('url', '').strip()

            print(f"\nğŸŒ Procesando URL ID {url_id}: {url}")

            # âœ… SOLUTION #3: TRY-CATCH INDIVIDUAL POR URL
            try:
                if not url:
                    raise Exception("URL vacÃ­a o nula")

                result = process_url(url)
                content = result.get("content", "").strip()

                if not content:
                    raise Exception("No se extrajo contenido de la URL")

                print(f"âœ… Contenido extraÃ­do ({result['type']}): {len(content)} caracteres")

                content_hash = hashlib.sha256(content.encode("utf-8")).hexdigest()

                # Verificar duplicados
                cursor.execute("""
                    SELECT COUNT(*) as count FROM training_urls
                    WHERE content_hash = %s AND indexed = 1 AND bot_id = %s
                """, (content_hash, bot_id))
                if cursor.fetchone()['count'] > 0:
                    print("â© Contenido duplicado, marcando como procesado (indexed=2)")
                    cursor.execute("UPDATE training_urls SET indexed = 2, status = 'processed' WHERE id = %s", (url_id,))
                    conn.commit()
                    continue

                # âœ… SOLUTION #4: METADATA COMPLETO - Chunking inteligente
                chunks = split_into_chunks(content, chunk_size=512, overlap=50, sentence_aware=True)
                print(f"   ğŸ“¦ Dividido en {len(chunks)} chunks")

                indexed_chunk_ids = []
                for chunk_idx, chunk in enumerate(chunks, 1):
                    chunk_qdrant_id = str(uuid.uuid4())
                    
                    # âœ… Payload con METADATA COMPLETO
                    payload = {
                        # Identificadores
                        "doc_id": url_id,
                        "bot_id": url_item.get('bot_id'),
                        "user_id": url_item.get('user_id'),
                        "bot_template_id": url_item.get('bot_template_id'),
                        
                        # Fuente y tipo
                        "source": "url",
                        "source_type": result.get("type", "unknown"),
                        "url": url,
                        
                        # Contenido
                        "original_text": chunk[:500],
                        "text_length": len(chunk),
                        "chunk_number": chunk_idx,
                        "total_chunks": len(chunks),
                        
                        # Metadata temporal
                        "processed_at": datetime.now().isoformat(),
                        "content_hash": content_hash,
                        "indexed_status": "indexed",
                        "error_message": None,
                    }
                    
                    # Agregar tags inferidos
                    tags = infer_tags_from_payload(payload, chunk)
                    payload.update(tags)

                    # Indexar chunk
                    client.upsert(
                        collection_name="voia_vectors",
                        points=[{
                            "id": chunk_qdrant_id,
                            "vector": get_embedding(chunk),
                            "payload": payload
                        }]
                    )
                    indexed_chunk_ids.append(chunk_qdrant_id)

                # Actualizar URL como indexada
                cursor.execute("""
                    UPDATE training_urls 
                    SET indexed = 1, status = 'processed', qdrant_id = %s, 
                        content_hash = %s, extracted_text = %s
                    WHERE id = %s
                """, (indexed_chunk_ids[0], content_hash, content[:10000], url_id))
                conn.commit()
                
                print(f"   âœ… {len(chunks)} chunks indexados")
                processed_count += 1

            except Exception as e:
                # âœ… SOLUTION #3: ERROR HANDLING - Marcar URL como fallida (indexed = -1)
                error_msg = str(e)[:500]
                print(f"âŒ Error en URL {url_id}: {error_msg}")
                failed_count += 1
                
                try:
                    cursor.execute("""
                        UPDATE training_urls 
                        SET indexed = -1, status = 'failed', error_message = %s
                        WHERE id = %s
                    """, (error_msg, url_id))
                    conn.commit()
                    print(f"   ğŸ“Œ Marcada como FALLIDA (indexed=-1), continuando batch...")
                except Exception as db_error:
                    print(f"   âš ï¸ Error actualizando DB: {db_error}")
                    conn.rollback()
                
                # âœ… CONTINUAR CON SIGUIENTE URL (NO FALLAR BATCH)
                continue

    finally:
        if 'conn' in locals() and conn.is_connected():
            cursor.close()
            conn.close()
        print(f"\nğŸ”š Procesamiento completado: {processed_count} exitosos, {failed_count} fallos")
# voia_vector_services/search_vectors.py
# from .embedder import get_embedding
# from .vector_store import get_or_create_vector_store

def search_vectors(bot_id: int, query: str = "", limit: int = 5):
    """
    Busca los vectores mÃ¡s relevantes para un bot dado y un query opcional.
    Si query estÃ¡ vacÃ­o, trae los top documentos del bot.
    """
    client = get_or_create_vector_store()
    vector = get_embedding(query) if query else None

    if vector:
        results = client.search(
            collection_name="voia_vectors",
            query_vector=vector,
            limit=limit,
            query_filter={
                "must": [{"key": "bot_id", "match": {"value": bot_id}}]
            }
        )
        return [r.payload for r in results]
    else:
        points, _ = client.scroll(collection_name="voia_vectors", limit=limit)
        return [p.payload for p in points if p.payload.get("bot_id") == bot_id]
import re
from typing import Dict, List

def infer_tags_from_payload(payload: Dict, extracted_text: str = "") -> Dict:
    tags = {}

    file_name = payload.get("file_name", "").lower()
    text = extracted_text.lower()

    def match_keywords(text: str, keywords: List[str]) -> bool:
        return any(re.search(rf"\b{re.escape(kw)}\b", text) for kw in keywords)

    # -----------------------------
    # Tipos de documento
    # -----------------------------
    tipos_documento = {
        "autorizaciÃ³n": ["autorizacion", "autorizaciÃ³n", "autorisacion", "autorizacÃ³n", "autorizaciÃ²n"],
        "contrato": ["contrato", "contarto", "contratp", "contratto"],
        "certificado": ["certificado", "certificdo", "cert", "constancia"],
        "factura": ["factura", "recibo", "cuenta de cobro"],
        "informaciÃ³n empresarial": ["informacion", "quienes somos", "perfil empresarial", "presentaciÃ³n", "empresa"]
    }

    for tipo, keywords in tipos_documento.items():
        if match_keywords(file_name, keywords) or match_keywords(text, keywords):
            tags["tipo"] = tipo
            break

    # -----------------------------
    # Temas
    # -----------------------------
    temas = {
        "nÃ³mina": ["nÃ³mina", "nomina", "descuento por nÃ³mina", "liquidaciÃ³n de nÃ³mina"],
        "salud": ["salud", "eps", "historia clÃ­nica", "centro mÃ©dico", "procedimiento mÃ©dico"],
        "financiero": ["prÃ©stamo", "cuota", "descuento", "interÃ©s", "deuda", "pago", "cartera"],
        "legal": ["demandas", "proceso judicial", "abogado", "juez", "cÃ³digo penal", "sentencia"],
        "educaciÃ³n": ["colegio", "universidad", "certificado de estudio", "boletÃ­n", "notas"],
        "laboral": ["trabajo", "empleo", "contrataciÃ³n", "vacaciones", "licencia"],
        "inmobiliario": ["arriendo", "inmueble", "propiedad", "contrato de arrendamiento"],
        "tecnologÃ­a": ["software", "sistema", "plataforma", "aplicaciÃ³n", "soporte tÃ©cnico"],
        "vehÃ­culos": ["vehÃ­culo", "soat", "licencia de conducciÃ³n", "revisiÃ³n tÃ©cnico-mecÃ¡nica", "matrÃ­cula vehicular"],
        "tributario": ["renta", "DIAN", "impuesto", "retenciÃ³n", "declaraciÃ³n"],
    }

    for tema, palabras in temas.items():
        if match_keywords(text, palabras):
            tags["tema"] = tema
            break

    # -----------------------------
    # Sectores econÃ³micos
    # -----------------------------
    sectores = {
        "tasaciÃ³n": ["tasaciÃ³n", "avaluo", "avalÃºo", "valor comercial", "peritaje", "inspecciÃ³n vehicular"],
        "automotriz": ["vehÃ­culos", "taller", "automotor", "siniestro", "accidente de trÃ¡nsito"],
        "salud": ["eps", "clÃ­nica", "mÃ©dico", "psicologÃ­a", "odontologÃ­a"],
        "educativo": ["universidad", "colegio", "instituciÃ³n educativa", "certificado acadÃ©mico"],
        "financiero": ["entidad financiera", "banco", "pago", "deuda", "cuenta"],
        "legal": ["tribunal", "juez", "proceso", "firma de abogados", "sentencia"],
        "tecnologÃ­a": ["startup", "aplicaciÃ³n", "plataforma digital", "software"],
        "logÃ­stica": ["transporte", "entrega", "mensajerÃ­a", "camiÃ³n"],
    }

    for sector, keywords in sectores.items():
        if match_keywords(text, keywords):
            if "sectores" not in tags:
                tags["sectores"] = []
            tags["sectores"].append(sector)

    # -----------------------------
    # Firma electrÃ³nica o fÃ­sica
    # -----------------------------
    firma_keywords = [
        "firma", "firmado", "firma electrÃ³nica", "firmado electrÃ³nicamente", "firma digital"
    ]
    if match_keywords(text, firma_keywords):
        tags["requiere_firma"] = True

    # -----------------------------
    # Especialidades mÃ©dicas (si aplica)
    # -----------------------------
    especialidades = {
        "psicologÃ­a": ["psicologÃ­a", "psicoterapia", "consulta psicolÃ³gica"],
        "odontologÃ­a": ["odontologÃ­a", "dentista", "odontograma"],
        "medicina general": ["consulta mÃ©dica", "mÃ©dico general", "valoraciÃ³n mÃ©dica"],
        "oftalmologÃ­a": ["oftalmologÃ­a", "visiÃ³n", "examen visual", "optometrÃ­a"],
    }

    for esp, keywords in especialidades.items():
        if match_keywords(text, keywords):
            tags["especialidad"] = esp
            break

    return tags
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct, PointIdsList
# from tag_utils import infer_tags_from_payload  # âœ…

COLLECTION_NAME = "voia_vectors"

client = QdrantClient(host="localhost", port=6333)


def get_or_create_vector_store():
    if not client.collection_exists(COLLECTION_NAME):
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=384, distance=Distance.COSINE)
        )
        print(f"ğŸ†• ColecciÃ³n '{COLLECTION_NAME}' creada.")
    else:
        print(f"âœ… ColecciÃ³n '{COLLECTION_NAME}' ya existe.")

    return client


def is_in_qdrant(qdrant_id: str) -> bool:
    try:
        points = client.retrieve(
            collection_name=COLLECTION_NAME,
            ids=[qdrant_id]
        )
        return len(points) > 0
    except Exception:
        return False


def add_point_to_qdrant(qdrant_id: str, vector: list, payload: dict = {}, extracted_text: str = ""):
    if is_in_qdrant(qdrant_id):
        print(f"â­ï¸ Punto con ID {qdrant_id} ya existe en Qdrant. No se insertarÃ¡ de nuevo.")
        return

    # ğŸ·ï¸ Agregar etiquetas inferidas al payload
    tags = infer_tags_from_payload(payload, extracted_text)
    payload.update(tags)

    client.upsert(
        collection_name=COLLECTION_NAME,
        points=[
            PointStruct(
                id=qdrant_id,
                vector=vector,
                payload=payload
            )
        ]
    )
    print(f"âœ… Punto {qdrant_id} insertado en Qdrant con etiquetas: {tags}")


def delete_point_from_qdrant(qdrant_id: str):
    try:
        client.delete(
            collection_name=COLLECTION_NAME,
            points_selector=PointIdsList([qdrant_id])
        )
        print(f"ğŸ—‘ï¸ Punto {qdrant_id} eliminado de Qdrant.")
    except Exception as e:
        print(f"âŒ Error al eliminar punto {qdrant_id}: {e}")


def list_all_points(limit=10):
    points, next_page = client.scroll(
        collection_name=COLLECTION_NAME,
        limit=limit
    )
    return points
